{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "require(\"ggplot2\")\n",
    "require(\"grid\")\n",
    "require(\"plyr\")\n",
    "library(reshape)\n",
    "library(ScottKnott)\n",
    "setwd(\".\")\n",
    "library(lda)\n",
    "library(tm)\n",
    "dataValues<- read.csv('MozillaCommentsFixed.csv',sep=\",\")\n",
    "dataValues=dataValues[sample(nrow(dataValues),size=1000,replace=FALSE),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim(dataValues)\n",
    "## Text Pre-processing.\n",
    "## Creating a Corpus from the Orginal Function\n",
    "## interprets each element of the vector x as a document\n",
    "CorpusObj<- VectorSource(dataValues$text);\n",
    "CorpusObj<-Corpus(CorpusObj);\n",
    "CorpusObj <- tm_map(CorpusObj, tolower) # convert all text to lower case\n",
    "CorpusObj <- tm_map(CorpusObj, removePunctuation) \n",
    "CorpusObj <- tm_map(CorpusObj, removeNumbers)\n",
    "CorpusObj <- tm_map(CorpusObj, removeWords, stopwords(\"english\")) \n",
    "CorpusObj <- tm_map(CorpusObj, stemDocument, language = \"english\") ## Stemming the words \n",
    "CorpusObj<-tm_map(CorpusObj,stripWhitespace)\n",
    "##create a term document matrix \n",
    "CorpusObj.tdm <- TermDocumentMatrix(CorpusObj, control = list(minWordLength = 3))\n",
    "inspect(CorpusObj.tdm[1:10,1:10])\n",
    "findFreqTerms(CorpusObj.tdm, lowfreq=1003)\n",
    "dim(CorpusObj.tdm)\n",
    "CorpusObj.tdm.sp <- removeSparseTerms(CorpusObj.tdm, sparse=0.88)\n",
    "dim(CorpusObj.tdm.sp)\n",
    "## Show Remining words per 15 Document.\n",
    "inspect(CorpusObj.tdm.sp[1:10,1:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## visualizing  the TD --  \n",
    "\n",
    "## Words Cloud Visualizing\n",
    "library(wordcloud)\n",
    "library(RColorBrewer)\n",
    "\n",
    "\n",
    "mTDM <- as.matrix(CorpusObj.tdm)\n",
    "v <- sort(rowSums(mTDM),decreasing=TRUE)\n",
    "d <- data.frame(word = names(v),freq=v)\n",
    "pal <- brewer.pal(9, \"BuGn\")\n",
    "pal <- pal[-(1:2)]\n",
    "png(\"wordcloud.png\", width=1280,height=800)\n",
    "wordcloud(d$word,d$freq, scale=c(8,.3),min.freq=2,max.words=100, random.order=T, rot.per=.15, colors=pal, vfont=c(\"sans serif\",\"plain\"))\n",
    "dev.off()\n",
    "\n",
    "CorpusObj.tdm.reformatted <- as.matrix(CorpusObj.tdm.sp)\n",
    "## Reformatting should -- checking memory space ..\n",
    "object.size(CorpusObj.tdm.sp)\n",
    "object.size(CorpusObj.tdm.reformatted)\n",
    "CorpusObj.tdm.reformatted = melt(CorpusObj.tdm.reformatted, value.name =\"count\")\n",
    "head(CorpusObj.tdm.reformatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(ggplot2)\n",
    "head(CorpusObj.tdm.reformatted)\n",
    "dim(CorpusObj.tdm.reformatted)\n",
    "## I have many Documents, so I limiting the number of Docs in X axis\n",
    "\n",
    "\n",
    "\n",
    "ggplot(CorpusObj.tdm.reformatted, aes(x = Docs, y = Terms, fill = log10( value) )) +\n",
    "  geom_tile(colour = \"white\") +\n",
    "  scale_fill_gradient(high=\"#FF0000\" , low=\"#FFFFFF\")+\n",
    "  ylab(\"\") +\n",
    "  theme(panel.background = element_blank()) +\n",
    "  theme(axis.text.x = element_blank(), axis.ticks.x = element_blank()); \n",
    "\n",
    "\n",
    "## Controlling Sparse Terms\n",
    "CorpusObj.tdm.sp <- removeSparseTerms(CorpusObj.tdm, sparse=0.5)\n",
    "## Convert document term matrix to data frame\n",
    "CorpusObj.tdm.sp.df <- as.data.frame(inspect(CorpusObj.tdm.sp ))\n",
    "## Number of words remaining\n",
    "nrow(CorpusObj.tdm.sp.df)\n",
    "\n",
    "require(slam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transpose document term matrix, necessary for the next steps using mean term \n",
    "#frequency-inverse document frequency (tf-idf) \n",
    "#to select the vocabulary for topic modeling\n",
    "CorpusObj.tdm.sp.t <- t(CorpusObj.tdm.sp)\n",
    "summary(col_sums(CorpusObj.tdm.sp.t))\n",
    "# calculate tf-idf values\n",
    "term_tfidf <- tapply(CorpusObj.tdm.sp.t$v/row_sums(CorpusObj.tdm.sp.t)[CorpusObj.tdm.sp.t$i], CorpusObj.tdm.sp.t$j,mean) * log2(nDocs(CorpusObj.tdm.sp.t)/col_sums(CorpusObj.tdm.sp.t>0))\n",
    "summary(term_tfidf)\n",
    "# keep only those terms that are slightly less frequent that the median\n",
    "CorpusObj.tdm.sp.t.tdif <- CorpusObj.tdm.sp.t[,term_tfidf>=1.0]\n",
    "CorpusObj.tdm.sp.t.tdif <- CorpusObj.tdm.sp.t[row_sums(CorpusObj.tdm.sp.t) > 0, ]\n",
    "summary(col_sums(CorpusObj.tdm.sp.t.tdif)) \n",
    "\n",
    "require(topicmodels)\n",
    "\n",
    "myModel=builtModel<-LDA(CorpusObj.tdm, 10);\n",
    "head(topics(myModel))\n",
    "\n",
    "\n",
    "best.model <- lapply(seq(2, 50, by = 1), function(d){LDA(CorpusObj.tdm.sp.t.tdif, d)}) # this will make a topic model for every number of topics between 2 and 50... it will take some time! \n",
    "best.model.logLik <- as.data.frame(as.matrix(lapply(best.model, logLik)))  # this will produce a list of logLiks for each model... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 5 words per Topics\n",
    "terms(myModel, 5 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Faster Way of doing LDA \n",
    "\n",
    "corpusLDA <- lexicalize(CorpusObj )\n",
    "require(lda)\n",
    "\n",
    "ldaModel=lda.collapsed.gibbs.sampler(corpusLDA$documents,K=10,vocab=corpusLDA$vocab,burnin=9999,num.iterations=1000,alpha=1,eta=0.1)\n",
    "top.words <- top.topic.words(ldaModel$topics, 5, by.score=TRUE)\n",
    "print(top.words) - \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you interested to know the membership of each topic per document for LDA package you can use : \n",
    "\n",
    "ldaModel=lda.collapsed.gibbs.sampler(corpusLDA$documents,K=100,vocab=corpusLDA$vocab,burnin=9999,num.iterations=1000,alpha=1,eta=0.1, compute.log.likelihood = TRUE)\n",
    "\n",
    "topic.proportions <- t(ldaModel$document_sums) / colSums(ldaModel$document_sums)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References:\n",
    "\n",
    "http://www.codemiles.com/r-examples/topic-modeling-using-lda-in-r-t11119.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
